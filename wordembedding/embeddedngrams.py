__author__ = "Patrick Nicolas"
__copyright__ = "Copyright 2020, 2022  All rights reserved."

import torch
import torch.nn as nn
import constants

"""
    Base class (PyTorch Module) for NGram wordembedding encoder_model (i.e. CBOW, Skip Gram....). 
    The NGram wordembedding is generated by parsing corpus for pre-defined relevant words/annotations
    and apply a traditional Word2Vec DL algorithm.
    The context is defined as 
        is_symmetric == false     word[encoder-n], word[encoder-n-1], ... word[encoder-1], target
        is_symmetric == true      word[encoder-n], word[encoder-n-1], ... word[encoder-1], target, word[encoder+1], ... word[encoder+n]
        
    :param vocab_size Size of vocab
    :param embedding_dim Size of the embedding layer
    :param context_size Size of the context 
    :param is_symmetric Defines the relation of the context words from the target
"""


class EmbeddedNGrams(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, context_size: int, is_symmetric: bool):
        super(EmbeddedNGrams, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim, sparse=False)
        dim = embedding_dim * context_size
        # If the context is symmetric..
        if is_symmetric:
            dim = dim * 2
        self.dim = dim

    @staticmethod
    def embed(input: torch.Tensor, embeddings_model: nn.Embedding) -> torch.Tensor:
        embeddings_model.weight = nn.Parameter(embeddings_model.weight.to(constants.torch_device))
        x = embeddings_model(input)
        return x.view((1, -1))

    def __repr__(self) -> str:
        return f'Vocabulary size: {self.dim}\n{repr(self.embeddings)}'